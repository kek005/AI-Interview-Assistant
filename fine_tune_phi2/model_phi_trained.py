import torch
import time
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# ğŸš¨ Fail if no GPU is available
if not torch.cuda.is_available():
    raise RuntimeError("ğŸš¨ No GPU detected! This model requires CUDA for real-time performance.")

# Set model path
MODEL_PATH = "./models/phi2_finetuned_azure_full"

# Force GPU execution
device = torch.device("cuda")

# ğŸ”¥ Enable 4-bit Quantization for Max Speed
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True, 
    bnb_4bit_compute_dtype=torch.float16  # âœ… Prevents dtype mismatch warning
)

# ğŸ”¥ Load Tokenizer with Fast Mode
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=True)

# ğŸ”¥ Load Model with Maximum Optimizations
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    quantization_config=quantization_config,
    torch_dtype=torch.float16,  # Switch to FP16 for extra speed
    attn_implementation="flash_attention_2",  # Optimized for RTX 40-series
    device_map="auto"
)

# ğŸ”¥ Compile Model for Even Faster Execution
model = torch.compile(model)

# âœ… Set pad_token_id to Avoid Warnings & Speed Up Execution
model.config.pad_token_id = tokenizer.eos_token_id

# âœ… Warm-up Inference (50 Trials)
print("\nğŸ”¥ Warming up model... (50 trials, results hidden)")
warmup_trials = 50
times = []
dummy_input = tokenizer("Question: What is Microsoft Entra ID?\nAnswer:", return_tensors="pt").to(device)

for _ in range(warmup_trials):
    start_time = time.time()
    _ = model.generate(**dummy_input, max_length=30, pad_token_id=tokenizer.eos_token_id, use_cache=True)
    print("warming up")
    end_time = time.time()
    times.append(end_time - start_time)

print(f"âœ… Warm-up complete. Avg speed: {sum(times) / warmup_trials:.4f} sec\n")

# ğŸ¤– AI Interview Assistant Interactive Mode
print("ğŸ¤– AI Interview Assistant is running... Type your question and press Enter.")
print("ğŸ”´ Type 'exit' to stop.\n")

while True:
    question = input("â“ Enter your question: ")
    if question.lower() == "exit":
        print("ğŸ›‘ Exiting AI Interview Assistant. Goodbye!")
        break
    
    # âœ… Prepare Input (Fixed `.to(device)`)
    inputs = tokenizer(f"Question: {question}\nAnswer:", return_tensors="pt").to(device)

    # â±ï¸ Measure Response Time
    start_time = time.time()
    outputs = model.generate(**inputs, max_length=70, pad_token_id=tokenizer.eos_token_id, use_cache=True)
    end_time = time.time()

    # âœ… Print Response
    print("\nğŸ¤– Model Response:", tokenizer.decode(outputs[0], skip_special_tokens=True))
    print(f"âš¡ Response Time: {end_time - start_time:.4f} sec\n")